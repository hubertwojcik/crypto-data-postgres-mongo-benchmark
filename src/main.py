import time
import os
from typing import List, Dict, Any

from src.config import *
from src.db.postgres_manager import PostgresManager
from src.db.mongo_manager import MongoManager
from src.etl.load_tweets import load_csv

from pymongo import InsertOne

BATCH_SIZE = int(os.getenv("BATCH_SIZE", "1000"))

def build_mongo_doc(row: Dict[str, Any]) -> Dict[str, Any]:
    """Buduje dokument dla jednej KOLEKCJI 'tweets' (zagnieÅ¼dÅ¼ony user)."""
    hashtags = row.get("hashtags") or []
    hashtags = [str(h).strip().lstrip("#").lower() for h in hashtags if str(h).strip()]
    doc = {
        "user": {
            "user_name": row.get("user_name"),
            "user_location": row.get("user_location"),
            "user_description": row.get("user_description"),
            "user_created": row.get("user_created"),
            "user_followers": row.get("user_followers"),
            "user_friends": row.get("user_friends"),
            "user_favourites": row.get("user_favourites"),
            "user_verified": bool(row.get("user_verified")) if row.get("user_verified") is not None else False,
        },
        "date": row.get("date"),
        "text": row.get("text"),
        "hashtags": hashtags,
        "source": row.get("source") or None,
        "is_retweet": bool(row.get("is_retweet")) if row.get("is_retweet") is not None else False,
    }
    return doc

def flush_postgres(pg: PostgresManager):
    """Commit dla Postgresa (dla spÃ³jnoÅ›ci logÃ³w)."""
    pg.commit()

def flush_mongo(mg: MongoManager, ops: List[InsertOne]) -> int:
    """WysyÅ‚a zebrane operacje do Mongo w jednym `bulk_write`."""
    if not ops:
        return 0
    res = mg.col.bulk_write(ops, ordered=False)
    return res.inserted_count

def analyze_csv_data(csv_path: str):
    """Przeanalizuj dane CSV i wyÅ›wietl statystyki."""
    print(f"ğŸ“Š Analiza danych z pliku: {csv_path}")
    
    import pandas as pd
    
    # Wczytaj dane do pandas DataFrame
    df = pd.read_csv(csv_path)
    
    print(f"\nğŸ“ˆ Podstawowe statystyki:")
    print(f"  Liczba rekordÃ³w: {len(df):,}")
    print(f"  Liczba kolumn: {len(df.columns)}")
    print(f"  Rozmiar pliku: {os.path.getsize(csv_path) / (1024*1024):.1f} MB")
    
    print(f"\nğŸ” Analiza kolumn:")
    for col in df.columns:
        null_count = df[col].isnull().sum()
        null_pct = (null_count / len(df)) * 100
        unique_count = df[col].nunique()
        
        print(f"  {col}:")
        print(f"    BrakujÄ…ce wartoÅ›ci: {null_count:,} ({null_pct:.1f}%)")
        print(f"    Unikalne wartoÅ›ci: {unique_count:,}")
        
        # PokaÅ¼ przykÅ‚adowe wartoÅ›ci dla pierwszych 5 kolumn
        if col in df.columns[:5]:
            sample_values = df[col].dropna().head(3).tolist()
            print(f"    PrzykÅ‚ady: {sample_values}")
    
    return df


def clean_csv_data(df):
    """WyczyÅ›Ä‡ i przygotuj dane CSV."""
    print(f"\nğŸ§¹ Czyszczenie danych...")
    
    original_count = len(df)
    
    # 1. UsuÅ„ duplikaty
    df_cleaned = df.drop_duplicates()
    duplicates_removed = original_count - len(df_cleaned)
    print(f"  UsuniÄ™to duplikatÃ³w: {duplicates_removed:,}")
    
    # 2. SprawdÅº i wyczyÅ›Ä‡ kolumny tekstowe
    text_columns = ['user_name', 'user_location', 'user_description', 'text']
    for col in text_columns:
        if col in df_cleaned.columns:
            # UsuÅ„ rekordy z pustymi stringami
            df_cleaned = df_cleaned[df_cleaned[col].fillna('').str.strip() != '']
            print(f"  WyczyÅ›ciono kolumnÄ™ {col}")
    
    # 3. SprawdÅº user_name - musi byÄ‡ unikalny i niepusty
    df_cleaned = df_cleaned.dropna(subset=['user_name'])
    df_cleaned = df_cleaned[df_cleaned['user_name'].str.strip() != '']
    
    # 4. SprawdÅº date - musi byÄ‡ prawidÅ‚owa data
    df_cleaned = df_cleaned.dropna(subset=['date'])
    
    # 5. SprawdÅº text - musi byÄ‡ niepusty
    df_cleaned = df_cleaned.dropna(subset=['text'])
    df_cleaned = df_cleaned[df_cleaned['text'].str.strip() != '']
    
    # 6. WyczyÅ›Ä‡ hashtags - usuÅ„ puste listy i nieprawidÅ‚owe formaty
    if 'hashtags' in df_cleaned.columns:
        # ZastÄ…p NaN pustymi listami
        df_cleaned['hashtags'] = df_cleaned['hashtags'].fillna('[]')
        # UsuÅ„ rekordy z pustymi listami hashtagÃ³w (opcjonalnie)
        # df_cleaned = df_cleaned[df_cleaned['hashtags'] != '[]']
    
    # 7. WyczyÅ›Ä‡ kolumny numeryczne
    numeric_columns = ['user_followers', 'user_friends', 'user_favourites']
    for col in numeric_columns:
        if col in df_cleaned.columns:
            # ZastÄ…p ujemne wartoÅ›ci 0
            df_cleaned[col] = df_cleaned[col].clip(lower=0)
            # ZastÄ…p NaN 0
            df_cleaned[col] = df_cleaned[col].fillna(0)
    
    # 8. WyczyÅ›Ä‡ kolumny boolean
    boolean_columns = ['user_verified', 'is_retweet']
    for col in boolean_columns:
        if col in df_cleaned.columns:
            # ZastÄ…p NaN False
            df_cleaned[col] = df_cleaned[col].fillna(False)
    
    # 9. WyczyÅ›Ä‡ source
    if 'source' in df_cleaned.columns:
        df_cleaned['source'] = df_cleaned['source'].fillna('Unknown')
    
    final_count = len(df_cleaned)
    removed_count = original_count - final_count
    
    print(f"  UsuniÄ™to rekordÃ³w: {removed_count:,}")
    print(f"  PozostaÅ‚o rekordÃ³w: {final_count:,}")
    print(f"  Procent zachowanych: {(final_count/original_count)*100:.1f}%")
    
    return df_cleaned


def validate_data_quality(df):
    """SprawdÅº jakoÅ›Ä‡ danych po czyszczeniu."""
    print(f"\nâœ… Walidacja jakoÅ›ci danych:")
    
    import pandas as pd
    
    issues = []
    
    # SprawdÅº czy user_name jest unikalny
    if df['user_name'].duplicated().any():
        issues.append("Duplikaty w user_name")
    else:
        print("  âœ… user_name jest unikalny")
    
    # SprawdÅº czy nie ma pustych tekstÃ³w
    text_cols = ['user_name', 'text']
    for col in text_cols:
        if (df[col].str.strip() == '').any():
            issues.append(f"Puste wartoÅ›ci w {col}")
        else:
            print(f"  âœ… {col} nie zawiera pustych wartoÅ›ci")
    
    # SprawdÅº czy daty sÄ… prawidÅ‚owe
    try:
        pd.to_datetime(df['date'], errors='raise')
        print("  âœ… Daty sÄ… prawidÅ‚owe")
    except:
        issues.append("NieprawidÅ‚owe daty")
    
    # SprawdÅº czy kolumny numeryczne sÄ… nieujemne
    numeric_cols = ['user_followers', 'user_friends', 'user_favourites']
    for col in numeric_cols:
        if col in df.columns:
            if (df[col] < 0).any():
                issues.append(f"Ujemne wartoÅ›ci w {col}")
            else:
                print(f"  âœ… {col} zawiera tylko nieujemne wartoÅ›ci")
    
    if issues:
        print(f"\nâš ï¸  Znalezione problemy:")
        for issue in issues:
            print(f"    - {issue}")
    else:
        print(f"\nğŸ‰ Wszystkie dane przeszÅ‚y walidacjÄ™!")
    
    return len(issues) == 0


def clear_databases(pg: PostgresManager, mg: MongoManager):
    """WyczyÅ›Ä‡ bazy danych przed Å‚adowaniem nowych danych."""
    print("\nğŸ§¹ Czyszczenie baz danych...")
    
    # PostgreSQL - usuÅ„ wszystkie tabele
    with pg.conn.cursor() as cur:
        cur.execute("DROP TABLE IF EXISTS tweet_hashtags CASCADE")
        cur.execute("DROP TABLE IF EXISTS tweets CASCADE")
        cur.execute("DROP TABLE IF EXISTS hashtags CASCADE")
        cur.execute("DROP TABLE IF EXISTS sources CASCADE")
        cur.execute("DROP TABLE IF EXISTS users CASCADE")
    pg.conn.commit()
    
    # MongoDB - usuÅ„ kolekcjÄ™
    mg.col.drop()
    
    print("âœ… Bazy danych wyczyszczone!")


def run_basic_benchmarks(pg: PostgresManager, mg: MongoManager):
    """Uruchom podstawowe testy wydajnoÅ›ci."""
    print("\nğŸ” Uruchamianie podstawowych benchmarkÃ³w...")
    
    # Test 1: Liczenie rekordÃ³w
    print("\nğŸ“Š Test 1: Liczenie rekordÃ³w")
    
    # PostgreSQL
    start = time.perf_counter()
    with pg.conn.cursor() as cur:
        cur.execute("SELECT COUNT(*) FROM tweets")
        pg_tweet_count = cur.fetchone()[0]
        cur.execute("SELECT COUNT(*) FROM users")
        pg_user_count = cur.fetchone()[0]
    pg_time = time.perf_counter() - start
    
    # MongoDB
    start = time.perf_counter()
    mongo_tweet_count = mg.col.count_documents({})
    mongo_user_count = len(mg.col.distinct("user.user_name"))
    mongo_time = time.perf_counter() - start
    
    print(f"  PostgreSQL: {pg_tweet_count} tweetÃ³w, {pg_user_count} uÅ¼ytkownikÃ³w w {pg_time:.4f}s")
    print(f"  MongoDB:    {mongo_tweet_count} tweetÃ³w, {mongo_user_count} uÅ¼ytkownikÃ³w w {mongo_time:.4f}s")
    
    # Test 2: Pobieranie najnowszych tweetÃ³w
    print("\nğŸ“Š Test 2: Najnowsze tweety (100 rekordÃ³w)")
    
    # PostgreSQL
    start = time.perf_counter()
    with pg.conn.cursor() as cur:
        cur.execute("""
            SELECT t.text, u.user_name, t.date 
            FROM tweets t 
            JOIN users u ON t.user_id = u.id 
            ORDER BY t.date DESC 
            LIMIT 100
        """)
        pg_recent = cur.fetchall()
    pg_time = time.perf_counter() - start
    
    # MongoDB
    start = time.perf_counter()
    mongo_recent = list(mg.col.find(
        {},
        {"text": 1, "user.user_name": 1, "date": 1}
    ).sort("date", -1).limit(100))
    mongo_time = time.perf_counter() - start
    
    print(f"  PostgreSQL: {len(pg_recent)} rekordÃ³w w {pg_time:.4f}s")
    print(f"  MongoDB:    {len(mongo_recent)} rekordÃ³w w {mongo_time:.4f}s")
    
    # Test 3: Wyszukiwanie po hashtagach
    print("\nğŸ“Š Test 3: Wyszukiwanie po hashtagach")
    
    # PostgreSQL
    start = time.perf_counter()
    with pg.conn.cursor() as cur:
        cur.execute("""
            SELECT t.text, u.user_name, h.tag
            FROM tweets t
            JOIN users u ON t.user_id = u.id
            JOIN tweet_hashtags th ON t.id = th.tweet_id
            JOIN hashtags h ON th.hashtag_id = h.id
            WHERE h.tag = 'bitcoin'
            ORDER BY t.date DESC
            LIMIT 50
        """)
        pg_hashtag = cur.fetchall()
    pg_time = time.perf_counter() - start
    
    # MongoDB
    start = time.perf_counter()
    mongo_hashtag = list(mg.col.find(
        {"hashtags": "bitcoin"},
        {"text": 1, "user.user_name": 1, "hashtags": 1}
    ).sort("date", -1).limit(50))
    mongo_time = time.perf_counter() - start
    
    print(f"  PostgreSQL: {len(pg_hashtag)} rekordÃ³w w {pg_time:.4f}s")
    print(f"  MongoDB:    {len(mongo_hashtag)} rekordÃ³w w {mongo_time:.4f}s")
    
    print("\nâœ… Podstawowe benchmarki zakoÅ„czone!")


def run():
    print("ğŸ”Œ ÅÄ…czenie z Postgres & Mongo...")
    pg = PostgresManager(PG_HOST, PG_PORT, PG_DB, PG_USER, PG_PASS)
    mg = MongoManager(MONGO_URI, MONGO_DB)

    # WyczyÅ›Ä‡ bazy danych
    clear_databases(pg, mg)

    print("ğŸ§± Inicjalizacja schematu / indeksÃ³w...")
    pg.init_schema()
    mg.init_indexes()  # indeksy: date, user.user_name, hashtags, is_retweet

    # Analiza i czyszczenie danych CSV
    print(f"\nğŸ“Š Analiza i czyszczenie danych CSV...")
    df_raw = analyze_csv_data(CSV_PATH)
    df_cleaned = clean_csv_data(df_raw)
    data_quality_ok = validate_data_quality(df_cleaned)
    
    if not data_quality_ok:
        print("âš ï¸  Dane zawierajÄ… problemy jakoÅ›ciowe. Kontynuujesz? (Enter aby kontynuowaÄ‡)")
        input()
    
    print(f"\nğŸ“¥ Wczytywanie wyczyszczonych danych...")
    # Konwertuj DataFrame z powrotem na generator
    rows_iter = (row.to_dict() for _, row in df_cleaned.iterrows())

    mongo_ops: List[InsertOne] = []
    total = 0
    t0 = time.perf_counter()

    for row in rows_iter:
        # ---------- PostgreSQL (model relacyjny) ----------
        user_id = pg.upsert_user(row)
        source_id = pg.get_or_create_source(row.get("source"))
        tweet_id = pg.insert_tweet(user_id, row, source_id)

        for tag in (row.get("hashtags") or []):
            tag = (str(tag) or "").strip().lstrip("#").lower()
            if not tag:
                continue
            hid = pg.get_or_create_hashtag(tag)
            pg.link_tweet_hashtag(tweet_id, hid)

        # ---------- MongoDB (1 kolekcja: 'tweets') ----------
        mongo_ops.append(InsertOne(build_mongo_doc(row)))

        total += 1
        if total % BATCH_SIZE == 0:
            # flush Postgres
            flush_postgres(pg)
            # flush Mongo
            inserted = flush_mongo(mg, mongo_ops)
            mongo_ops.clear()
            print(f"âœ… Batch {total // BATCH_SIZE}: zapisano {BATCH_SIZE} rekordÃ³w (Mongo inserted={inserted}).")

    # final flush
    flush_postgres(pg)
    if mongo_ops:
        inserted = flush_mongo(mg, mongo_ops)
        mongo_ops.clear()
        print(f"âœ… Finalny batch: Mongo inserted={inserted}")

    elapsed = time.perf_counter() - t0
    print(f"ğŸ‰ Gotowe! ZaÅ‚adowano Å‚Ä…cznie {total} rekordÃ³w w {elapsed:.2f}s.")

    # Uruchom podstawowe benchmarki
    run_basic_benchmarks(pg, mg)

    # zamykanie
    pg.close()
    mg.close()

if __name__ == "__main__":
    run()
